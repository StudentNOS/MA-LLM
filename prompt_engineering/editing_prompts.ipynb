{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: markovify in ./.venv/lib/python3.11/site-packages (0.9.4)\n",
      "Requirement already satisfied: unidecode in ./.venv/lib/python3.11/site-packages (from markovify) (1.3.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install markovify\n",
    "import markovify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('prompts.csv', delimiter=';')\n",
    "\n",
    "# Function to replace 'title' with 'abstract' in a string\n",
    "def replace_title_with_abstract(text):\n",
    "    return text.replace('title', 'abstract').replace('Title', 'Abstract')\n",
    "\n",
    "# Copy TitlePrompt to AbstractPrompt and replace 'title' with 'abstract'\n",
    "df['AbstractPrompt'] = df['TitlePrompt'].apply(lambda x: replace_title_with_abstract(x) if pd.notnull(x) else x)\n",
    "\n",
    "# Save the modified DataFrame back to CSV\n",
    "df.to_csv('prompts.csv', index=False, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            TitlePrompt  Words  Sentences\n",
      "0                                                   NaN      0          0\n",
      "1           Screen the titles below like a human would.      8          1\n",
      "2            Screen the title below like a human would.      8          1\n",
      "3            You are a world-class clinical researcher.      6          1\n",
      "4     Do not exclude any titles unless you are absol...     13          1\n",
      "...                                                 ...    ...        ...\n",
      "1016                                                NaN      0          0\n",
      "1017                                                NaN      0          0\n",
      "1018                                                NaN      0          0\n",
      "1019                                                NaN      0          0\n",
      "1020                                                NaN      0          0\n",
      "\n",
      "[1021 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Function to count words and sentences\n",
    "def count_words_and_sentences(text):\n",
    "    if pd.isnull(text):\n",
    "        return [0, 0]\n",
    "    words = len(text.split())\n",
    "    sentences = len(re.findall(r'[.!?]', text))\n",
    "    return [words, sentences]\n",
    "\n",
    "# Apply the function to the TitlePrompt column and update the Words and Sentences columns\n",
    "df[['Words', 'Sentences']] = df['TitlePrompt'].apply(lambda x: pd.Series(count_words_and_sentences(x)))\n",
    "\n",
    "print(df[['TitlePrompt', 'Words', 'Sentences']])\n",
    "df.to_csv('prompts.csv', index=False, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a researcher in a landmark systematic review—select the most impactful papers.\n",
      "Use your extensive scientific expertise to distinguish between relevant and exclude those that are irrelevant.\n",
      "You are a meticulous researcher tasked with screening potential papers for their relevance in the meta-analysis.\n",
      "Retain titles that explicitly state data availability, handling of missing data, and measurable outcomes that align with multi-disciplinary standards.\n",
      "Exclude studies with high-impact potential.\n",
      "Include only those titles that are irrelevant.\n",
      "You are familiar with the mindset of a quality-control expert ensuring compliance with systematic review for a systematic review.\n",
      "You are a researcher in a landmark systematic review—select the most impactful papers.\n",
      "You are familiar with the concepts of sensitivity and specificity to ensure a balanced selection process.\n",
      "Aim to keep studies with incomplete design details or ambiguous methodologies.\n"
     ]
    }
   ],
   "source": [
    "# Get raw text as string.\n",
    "text = ' '.join(df['TitlePrompt'].dropna().tolist())\n",
    "\n",
    "# Build the model.\n",
    "text_model = markovify.Text(text)\n",
    "\n",
    "# Print five randomly-generated sentences\n",
    "for i in range(10):\n",
    "    print(text_model.make_sentence())\n",
    "\n",
    "# Print three randomly-generated sentences of no more than 280 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxonomie = {\n",
    "    \"Role\": {\n",
    "      \"Scientific_Roles\": [\n",
    "        \"Meta-Analyst\",\n",
    "        \"Clinical Researcher\",\n",
    "        \"Systematic Reviewer\",\n",
    "        \"Senior Editor\",\n",
    "        \"Peer-Reviewer\",\n",
    "        \"Researcher\",\n",
    "        \"Data Scientist\",\n",
    "        \"Journal Editor\",\n",
    "        \"Interdisciplinary Team Leader\",\n",
    "        \"Developer of Machine-Learning Datasets\",\n",
    "        \"part of a peer-review team\",\n",
    "        \"tasked with\",\n",
    "      ],\n",
    "      \"Adjectives\": [\n",
    "        \"\",\n",
    "        \"world-class\",\n",
    "        \"meticulous\",\n",
    "        \"experienced\",\n",
    "        \"knowledgeable\",\n",
    "        \"innovative\",\n",
    "        \"dedicated\",\n",
    "        \"passionate\",\n",
    "        \"effective\",\n",
    "        \"collaborative\",\n",
    "        \"ethical\",\n",
    "        \"transparent\",\n",
    "        \"reliable\",\n",
    "        \"responsible\",\n",
    "        \"professional\",\n",
    "        \"efficient\",\n",
    "        \"systematic\",\n",
    "        \"methodical\",\n",
    "        \"analytical\",\n",
    "        \"critical\",\n",
    "        \"creative\",\n",
    "        \"motivated\",\n",
    "        \"engaged\"\n",
    "      ],\n",
    "      \"Noun\": [\n",
    "        \"you are\",\n",
    "        \"you are a\",\n",
    "        \"\"\n",
    "      ],\n",
    "      \"Verb\": [\n",
    "        \"\",\n",
    "        \"Pretend\",\n",
    "        \"Assume\",\n",
    "        \"Imagine\",\n",
    "        \"Suppose\",\n",
    "        \"Evaluate the relevance of the titles below with the mindset of a\"\n",
    "      ],\n",
    "      \"Additional_Information\": [\n",
    "        \"\",\n",
    "        \"assessing studies for systematic reviews\",\n",
    "        \"curating studies for a meta-analysis publication\",\n",
    "        \"collaborating with an interdisciplinary team of scientists, clinicians, and statisticians\",\n",
    "        \"in systematic review methods\",\n",
    "        \"in meta-analysis techniques\",\n",
    "        \"in evidence-based medicine\",\n",
    "        \"in data analysis and interpretation\",\n",
    "        \"in statistical methods\",\n",
    "        \"in clinical research\",\n",
    "        \"curating studies for a meta-analysis publication\",\n",
    "        \"analyzing data for clinical trials\",\n",
    "        \"developing research protocols and methodologies\",\n",
    "        \"writing and reviewing scientific manuscripts\",\n",
    "        \"conducting literature searches and data extraction\",\n",
    "        \"conducting a systematic review\",\n",
    "        \"presenting findings at conferences and seminars\",\n",
    "        \"providing statistical support for research projects\",\n",
    "        \"ensuring compliance with ethical guidelines and standards\",\n",
    "        \"mentoring junior researchers and students\",\n",
    "        \"collaborating with industry partners and stakeholders\",\n",
    "        \"specializing in systematic reviews\",\n",
    "        \"assembling a gold-standard dataset for machine learning model training\",\n",
    "        \"in a world-class scientific team that wants to write a meta-analysis\",\n",
    "        \"evaluating titles for relevance to meta-analysis topics\",\n",
    "        \"submitting a meta-analysis to a high-impact journal\",\n",
    "        \"evaluating submissions for publication\",\n",
    "        \"evaluating a collection of academic papers to determine their relevance for inclusion in a meta-analysis\",\n",
    "        \"writing a grant proposal to secure funding for a systematic review\",\n",
    "        \"evaluating studies for compliance with ethical and regulatory standards in clinical trials\",\n",
    "      ]\n",
    "    }\n",
    "    \n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are Meta-Analyst\n",
      "You are Meta-Analyst assessing studies for systematic reviews\n",
      "You are Meta-Analyst curating studies for a meta-analysis publication\n",
      "You are Meta-Analyst collaborating with an interdisciplinary team of scientists, clinicians, and statisticians\n",
      "You are Meta-Analyst in systematic review methods\n",
      "You are Meta-Analyst in meta-analysis techniques\n",
      "You are Meta-Analyst in evidence-based medicine\n",
      "You are Meta-Analyst in data analysis and interpretation\n",
      "You are Meta-Analyst in statistical methods\n",
      "You are Meta-Analyst in clinical research\n",
      "Total number of generated sentences: 124200\n"
     ]
    }
   ],
   "source": [
    "# Function to generate sentences based on the given structure\n",
    "def generate_sentences(taxonomie):\n",
    "    sentences = []\n",
    "    for verb in taxonomie[\"Role\"][\"Verb\"]:\n",
    "        for noun in taxonomie[\"Role\"][\"Noun\"]:\n",
    "            for adjective in taxonomie[\"Role\"][\"Adjectives\"]:\n",
    "                for role in taxonomie[\"Role\"][\"Scientific_Roles\"]:\n",
    "                    for info in taxonomie[\"Role\"][\"Additional_Information\"]:\n",
    "                        sentence = f\"{verb} {noun} {adjective} {role} {info}\"\n",
    "                        # Replace multiple spaces with a single space and strip leading/trailing spaces\n",
    "                        sentence = re.sub(' +', ' ', sentence).strip()\n",
    "                        sentences.append(sentence)\n",
    "    return sentences\n",
    "\n",
    "# Generate sentences\n",
    "sentences = generate_sentences(taxonomie)\n",
    "\n",
    "# Print a few example sentences\n",
    "for sentence in sentences[:10]:\n",
    "    print(sentence)\n",
    "\n",
    "# Print the total number of generated sentences\n",
    "print(\"Total number of generated sentences:\", len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting language-tool-python\n",
      "  Downloading language_tool_python-2.8.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: pip in ./.venv/lib/python3.11/site-packages (from language-tool-python) (24.3.1)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.11/site-packages (from language-tool-python) (2.32.3)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.11/site-packages (from language-tool-python) (4.67.0)\n",
      "Collecting wheel (from language-tool-python)\n",
      "  Using cached wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests->language-tool-python) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.11/site-packages (from requests->language-tool-python) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests->language-tool-python) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests->language-tool-python) (2024.8.30)\n",
      "Downloading language_tool_python-2.8.1-py3-none-any.whl (35 kB)\n",
      "Using cached wheel-0.45.1-py3-none-any.whl (72 kB)\n",
      "Installing collected packages: wheel, language-tool-python\n",
      "Successfully installed language-tool-python-2.8.1 wheel-0.45.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading LanguageTool 6.4: 100%|██████████| 246M/246M [00:43<00:00, 5.68MB/s] \n",
      "Unzipping /var/folders/qg/82ll9gvd1pd4329hnkdpf8ch0000gn/T/tmppj46ata7.zip to /Users/canis/.cache/language_tool_python.\n",
      "Downloaded https://www.languagetool.org/download/LanguageTool-6.4.zip to /Users/canis/.cache/language_tool_python.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of valid sentences: 0\n"
     ]
    }
   ],
   "source": [
    "%pip install language-tool-python\n",
    "import language_tool_python\n",
    "\n",
    "# Initialize the LanguageTool object\n",
    "tool = language_tool_python.LanguageTool('en-US')\n",
    "\n",
    "# Function to check if a sentence makes sense\n",
    "def check_sentence(sentence):\n",
    "    matches = tool.check(sentence)\n",
    "    return len(matches) == 0\n",
    "\n",
    "# Apply the function to the generated sentences\n",
    "valid_sentences = [sentence for sentence in filtered_sentences if check_sentence(sentence)]\n",
    "\n",
    "# Print the valid sentences\n",
    "for sentence in valid_sentences[:10]:  # Print first 10 for example\n",
    "    print(sentence)\n",
    "\n",
    "# Print the total number of valid sentences\n",
    "print(\"Total number of valid sentences:\", len(valid_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Save the generated sentences to a text file\n",
    "with open('generated_sentences.txt', 'w') as f:\n",
    "    for sentence in sentences:\n",
    "        f.write(sentence + '\\n')\n",
    "\n",
    "# Load the sentences from the text file\n",
    "with open('generated_sentences.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Build the model\n",
    "text_model = markovify.Text(text)\n",
    "\n",
    "# Print five randomly-generated sentences\n",
    "for i in range(5):\n",
    "    print(text_model.make_sentence())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After generating the sentences for the role rule-based, we need to delete some combinations that do not make sense. Specifically, we will delete sentences that contain any of the following:\n",
    "\n",
    "1. \"Evaluate the relevance of the titles below with the mindset of a\" followed by \"you\".\n",
    "2. \"you are\" followed by any of the {Scientific_Role} except \"part of a peer-review team\" and \"tasked with\".\n",
    "3. \"you are\" followed by any {Adjective}.\n",
    "4. \"tasked with\" followed by \"in\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of filtered sentences: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the conditions for deletion\n",
    "def should_delete(sentence):\n",
    "    if re.search(r\"Evaluate the relevance of the titles below with the mindset of a.*\\byou\\b\", sentence):\n",
    "        return True\n",
    "    if re.search(r\"\\bare\\b.*\\b(Meta-Analyst|Clinical Researcher|Systematic Reviewer|Senior Editor|Peer-Reviewer|Researcher|Data Scientist|Journal Editor|Interdisciplinary Team Leader|Developer of Machine-Learning Datasets)\\b\", sentence):\n",
    "        return True\n",
    "    if not re.search(r\"\\bare\\b\", sentence):\n",
    "        return True\n",
    "    if re.search(r\"\\bare\\b.*\\b(\" + \"|\".join(taxonomie[\"Role\"][\"Adjectives\"]) + r\")\\b\", sentence):\n",
    "        return True\n",
    "    if re.search(r\"\\btasked with\\b.*\\bin\\b\", sentence):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "# Filter out the sentences that should be deleted\n",
    "filtered_sentences = [sentence for sentence in sentences if not should_delete(sentence)]\n",
    "\n",
    "# Print the filtered sentences\n",
    "for sentence in filtered_sentences[:10]:  # Print first 10 for example\n",
    "    print(sentence)\n",
    "\n",
    "# Print the total number of filtered sentences\n",
    "print(\"Total number of filtered sentences:\", len(filtered_sentences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3154928950.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[75], line 5\u001b[0;36m\u001b[0m\n\u001b[0;31m    for verb in taxonomie[\"Role\"][\"Verb\"]:\u001b[0m\n\u001b[0m                                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Generierung von Schnipseln\n",
    "def generate_snippets(taxonomie):\n",
    "    snippets = {\n",
    "        \"Role\": [f\"{verb} {noun} {adjective} {role} {info}.\" \n",
    "                 for verb in taxonomie[\"Role\"][\"Verb\"]:\n",
    "for noun in taxonomie[\"Role\"][\"Noun\"]:\n",
    "                        for adjective in taxonomie[\"Role\"][\"Adjectives\"]:\n",
    "                            for role in taxonomie[\"Role\"][\"Scientific_Roles\"]:\n",
    "                                for info in taxonomie[\"Role\"][\"Additional_Information\"]],\n",
    "        # \"Objective\": [f\"Your task is to {objective}.\" for objective in taxonomie[\"Objective\"]],\n",
    "        # \"ContentFocus\": [f\"Focus on {focus}.\" for focus in taxonomie[\"ContentFocus\"]],\n",
    "        # \"Style\": [f\"Ensure the tone is {style}.\" for style in taxonomie[\"Style\"]],\n",
    "        # \"Criteria\": [f\"Apply the following criteria: {criteria}.\" for criteria in taxonomie[\"Criteria\"]],\n",
    "        # \"Context\": [f\"Consider the context of {context}.\" for context in taxonomie[\"Context\"]]\n",
    "    }\n",
    "    return snippets\n",
    "\n",
    "snippets = generate_snippets(taxonomie)\n",
    "\n",
    "# JSON-Struktur\n",
    "snippets_json = json.dumps(snippets, indent=4)\n",
    "print(\"Generated JSON structure:\\n\", snippets_json)\n",
    "\n",
    "# Template-Erstellung\n",
    "template_variations = [\n",
    "    #\"{context} {role} {objective} {criteria} {content_focus} {style}\",\n",
    "    \"{role}.\", # {objective}. {content_focus}. {criteria}. {context}. {style}.\",\n",
    "    #\"Suppose {role}. {objective}. Focus on {content_focus}. Ensure {style}. {criteria}. Context: {context}.\"\n",
    "]\n",
    "\n",
    "# Funktion für die Variation von Templates\n",
    "def generate_template(role, objective, content_focus, criteria, context, style):\n",
    "    template = random.choice(template_variations)\n",
    "    return template.format(\n",
    "        role=role,\n",
    "    #     objective=objective,\n",
    "    #     content_focus=content_focus,\n",
    "    #     criteria=criteria,\n",
    "    #     context=context,\n",
    "    #     style=style\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (44012658.py, line 231)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[79], line 231\u001b[0;36m\u001b[0m\n\u001b[0;31m    }\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "\n",
    "taxonomy = {\n",
    "    \"Objective\": {\n",
    "      \"Verb\": [\n",
    "        \"select\",\n",
    "        \"screen\",\n",
    "        \"evaluate\",\n",
    "        \"\"\n",
    "      ],\n",
    "      \"Noun\": [\n",
    "          \"the titles\",\n",
    "          \"the title\",\n",
    "          \"those titles\",\n",
    "          \"these titles\",\n",
    "          \n",
    "\n",
    "\n",
    "\n",
    "        \"Select relevant titles\",\n",
    "        \"Exclude irrelevant titles\",\n",
    "        \"Ensure sensitivity and specificity\",\n",
    "        \"Ensure precision and recall\",\n",
    "        \"Focus on methodological rigor\"\n",
    "        \"Prepare for a meta-analysis\",\n",
    "        \"Develop clinical guidelines\",\n",
    "        \"Curate for public health reports\"\n",
    "        \"Demonstrate best practices\",\n",
    "        \"Train junior researchers\",\n",
    "        \"Support clinical decisions\",\n",
    "        \"Develop evidence-based guidelines\",\n",
    "        \"Select studies\",\n",
    "      ],\n",
    "      \"Adjective\": [\n",
    "        \"Select\",\n",
    "        \"Exclude\",\n",
    "        \"Ensure sensitivity and specificity by selecting\",\n",
    "        \"Ensure precision and recall by selecting\",\n",
    "        \"Focus on methodological rigor\"\n",
    "        \"Prepare for a meta-analysis\",\n",
    "        \"Develop clinical guidelines\",\n",
    "        \"Curate for public health reports\"\n",
    "        \"Demonstrate best practices\",\n",
    "        \"Train junior researchers\",\n",
    "        \"Support clinical decisions\",\n",
    "        \"Develop evidence-based guidelines\",\n",
    "        \"Select studies\",\n",
    "      ],\n",
    "      \"Verb\": [\n",
    "        \"Select relevant titles\",\n",
    "        \"Exclude irrelevant titles\",\n",
    "        \"Ensure sensitivity and specificity\",\n",
    "        \"Ensure precision and recall\",\n",
    "        \"Focus on methodological rigor\"\n",
    "        \"Prepare for a meta-analysis\",\n",
    "        \"Develop clinical guidelines\",\n",
    "        \"Curate for public health reports\"\n",
    "        \"Demonstrate best practices\",\n",
    "        \"Train junior researchers\",\n",
    "        \"Support clinical decisions\",\n",
    "        \"Develop evidence-based guidelines\",\n",
    "        \"Select studies\",\n",
    "      ],\n",
    "\n",
    "    \"Content Focus\": {\n",
    "      \"Sections of Meta-Analysis\": [\n",
    "        \"Title\",\n",
    "        \"Hypothesis/Research Question\",\n",
    "        \"Methods\",\n",
    "        \"Results\"\n",
    "      ],\n",
    "      \"Study Topics\": [\n",
    "        \"Specific research field\",\n",
    "        \"Target population\",\n",
    "        \"Interventions\"\n",
    "      ],\n",
    "      \"Methodological Details\": [\n",
    "        \"Study design\",\n",
    "        \"Statistical rigor\",\n",
    "        \"Transparency of methods\"\n",
    "      ],\n",
    "      \"Outcomes\": [\n",
    "        \"Primary and secondary endpoints\",\n",
    "        \"Relevance of results\",\n",
    "        \"Reproducibility\"\n",
    "      ]\n",
    "    },\n",
    "    \"Style\": {\n",
    "      \"Tone\": [\n",
    "        \"Formal and precise\",\n",
    "        \"Motivating and educational\",\n",
    "        \"Strict and demanding\"\n",
    "      ],\n",
    "      \"Structure\": [\n",
    "        \"Imperative\",\n",
    "        \"Hypothetical\",\n",
    "        \"Open-ended questions\",\n",
    "        \"Closed-ended questions\"\n",
    "      ],\n",
    "      \"Complexity\": [\n",
    "        \"Simple\",\n",
    "        \"Complex\"\n",
    "      ]\n",
    "    },\n",
    "    \"Criteria\": {\n",
    "      \"Inclusion Criteria\": [\n",
    "        \"Population\",\n",
    "        \"Intervention\",\n",
    "        \"Comparison\",\n",
    "        \"Outcomes\",\n",
    "        \"Study design\",\n",
    "        \"Topic relevance\"\n",
    "      ],\n",
    "      \"Exclusion Criteria\": [\n",
    "        \"No clear outcomes\",\n",
    "        \"Methodological weaknesses\",\n",
    "        \"Ethical concerns\",\n",
    "        \"Language barriers\",\n",
    "        \"Incomplete or unavailable data\"\n",
    "      ]\n",
    "    },\n",
    "    \"Context\": {\n",
    "      \"Meta-Analysis Related\": [\n",
    "        \"Preparing a high-level review\",\n",
    "        \"Training for systematic reviews\"\n",
    "      ],\n",
    "      \"Practice-Oriented\": [\n",
    "        \"Developing evidence-based guidelines\",\n",
    "        \"Supporting clinical decisions\"\n",
    "      ],\n",
    "      \"Educational-Oriented\": [\n",
    "        \"Demonstrating best practices\",\n",
    "        \"Training and mentoring\"\n",
    "      ],\n",
    "      \"Policy-Oriented\": [\n",
    "        \"Curating for public health initiatives\",\n",
    "        \"Selecting studies for policy reports\"\n",
    "      ]\n",
    "    },\n",
    "    \"Methodology\": {\n",
    "      \"Approach\": [\n",
    "        \"Signal-detection theory\",\n",
    "        \"Evidence-based clinical guidelines\",\n",
    "        \"Statistical frameworks\"\n",
    "      ],\n",
    "      \"Validation\": [\n",
    "        \"Reproducibility of methods\",\n",
    "        \"Transparency in reporting\",\n",
    "        \"Bias reduction measures\"\n",
    "      ]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Existierende Prompts\n",
    "existing_prompts = [\n",
    "    \"Screen the titles below like a human would.\",\n",
    "    \"You are a world-class clinical researcher.\",\n",
    "    \"Select the most relevant titles below.\",\n",
    "    \"Imagine conducting a systematic review. Choose only those titles that align closely with pre-defined eligibility criteria.\",\n",
    "    \"Pretend you are submitting a meta-analysis to a high-impact journal. Select titles based on strict inclusion criteria to ensure scientific rigor.\"\n",
    "]\n",
    "\n",
    "# Generierung von Schnipseln\n",
    "def generate_snippets(taxonomie):\n",
    "    snippets = {\n",
    "        \"Role\": [f\"Imagine you are a {role}.\" for role in taxonomie[\"Role\"]],\n",
    "        \"Objective\": [f\"Your task is to {objective}.\" for objective in taxonomie[\"Objective\"]],\n",
    "        \"ContentFocus\": [f\"Focus on {focus}.\" for focus in taxonomie[\"ContentFocus\"]],\n",
    "        \"Style\": [f\"Ensure the tone is {style}.\" for style in taxonomie[\"Style\"]],\n",
    "        \"Criteria\": [f\"Apply the following criteria: {criteria}.\" for criteria in taxonomie[\"Criteria\"]],\n",
    "        \"Context\": [f\"Consider the context of {context}.\" for context in taxonomie[\"Context\"]]\n",
    "    }\n",
    "    return snippets\n",
    "\n",
    "snippets = generate_snippets(taxonomie)\n",
    "\n",
    "# Ergänzung durch existierende Prompts\n",
    "for category in snippets:\n",
    "    snippets[category].extend(existing_prompts)\n",
    "\n",
    "# JSON-Struktur\n",
    "snippets_json = json.dumps(snippets, indent=4)\n",
    "print(\"Generated JSON structure:\\n\", snippets_json)\n",
    "\n",
    "# Template-Erstellung\n",
    "template_variations = [\n",
    "    \"{context} {role} {objective} {criteria} {content_focus} {style}\",\n",
    "    \"{role}. {objective}. {content_focus}. {criteria}. {context}. {style}.\",\n",
    "    \"Suppose {role}. {objective}. Focus on {content_focus}. Ensure {style}. {criteria}. Context: {context}.\"\n",
    "]\n",
    "\n",
    "# Funktion für die Variation von Templates\n",
    "def generate_template(role, objective, content_focus, criteria, context, style):\n",
    "    template = random.choice(template_variations)\n",
    "    return template.format(\n",
    "        role=role,\n",
    "        objective=objective,\n",
    "        content_focus=content_focus,\n",
    "        criteria=criteria,\n",
    "        context=context,\n",
    "        style=style\n",
    "    )\n",
    "\n",
    "# Test: Generierung einer Beispiel-Template\n",
    "example_template = generate_template(\n",
    "    role=\"Meta-Analyst\",\n",
    "    objective=\"Select impactful titles\",\n",
    "    content_focus=\"Evaluate mesenchymal stem cell therapy studies\",\n",
    "    criteria=\"Include only studies with clear outcomes\",\n",
    "    context=\"Systematic review preparation\",\n",
    "    style=\"Formal and precise\"\n",
    ")\n",
    "\n",
    "print(\"\\nGenerated Example Template:\\n\", example_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in ./.venv/lib/python3.11/site-packages (4.48.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in ./.venv/lib/python3.11/site-packages (from transformers) (0.27.1)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.11/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.11/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.venv/lib/python3.11/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./.venv/lib/python3.11/site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.11/site-packages (from transformers) (4.67.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.11/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests->transformers) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "At least one of TensorFlow 2.0 or PyTorch should be installed. To install TensorFlow 2.0, read the instructions at https://www.tensorflow.org/install/ To install PyTorch, read the instructions at https://pytorch.org/.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Lade ein vortrainiertes Modell zum Paraphrasieren\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m paraphraser \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext2text-generation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mt5-small\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Liste von Prompts\u001b[39;00m\n\u001b[1;32m      8\u001b[0m prompts \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTitlePrompt\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdropna()\u001b[38;5;241m.\u001b[39mtolist())\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-Persönlich/programme/coding/github/ensure/.venv/lib/python3.11/site-packages/transformers/pipelines/__init__.py:940\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    939\u001b[0m     model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[0;32m--> 940\u001b[0m     framework, model \u001b[38;5;241m=\u001b[39m \u001b[43minfer_framework_load_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    950\u001b[0m model_config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n\u001b[1;32m    951\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-Persönlich/programme/coding/github/ensure/.venv/lib/python3.11/site-packages/transformers/pipelines/base.py:240\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[0;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;124;03mSelect framework (TensorFlow or PyTorch) to use from the `model` passed. Returns a tuple (framework, model).\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;124;03m    `Tuple`: A tuple framework, model.\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tf_available() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_available():\n\u001b[0;32m--> 240\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAt least one of TensorFlow 2.0 or PyTorch should be installed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    242\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo install TensorFlow 2.0, read the instructions at https://www.tensorflow.org/install/ \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo install PyTorch, read the instructions at https://pytorch.org/.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    244\u001b[0m     )\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    246\u001b[0m     model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_pipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m task\n",
      "\u001b[0;31mRuntimeError\u001b[0m: At least one of TensorFlow 2.0 or PyTorch should be installed. To install TensorFlow 2.0, read the instructions at https://www.tensorflow.org/install/ To install PyTorch, read the instructions at https://pytorch.org/."
     ]
    }
   ],
   "source": [
    "%pip install transformers\n",
    "from transformers import pipeline\n",
    "\n",
    "# Lade ein vortrainiertes Modell zum Paraphrasieren\n",
    "paraphraser = pipeline(\"text2text-generation\", model=\"t5-small\")\n",
    "\n",
    "# Liste von Prompts\n",
    "prompts = ' '.join(df['TitlePrompt'].dropna().tolist())\n",
    "\n",
    "# Paraphrasierungen generieren\n",
    "variations = []\n",
    "for prompt in prompts:\n",
    "    output = paraphraser(f\"paraphrase: {prompt}\", max_length=50, num_return_sequences=5)\n",
    "    variations.extend([item['generated_text'] for item in output])\n",
    "\n",
    "# Variationen speichern\n",
    "with open(\"prompt_variationen.txt\", \"w\") as file:\n",
    "    file.write(\"\\n\".join(variations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.5.1-cp311-none-macosx_11_0_arm64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.venv/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Collecting networkx (from torch)\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.11/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.11/site-packages (from torch) (2024.12.0)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.11/site-packages (from jinja2->torch) (3.0.2)\n",
      "Downloading torch-2.5.1-cp311-none-macosx_11_0_arm64.whl (63.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.9/63.9 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: mpmath, sympy, networkx, torch\n",
      "Successfully installed mpmath-1.3.0 networkx-3.4.2 sympy-1.13.1 torch-2.5.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nAutoModelForSequenceClassification requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[93], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistilbert-base-uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     11\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[0;32m---> 12\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForSequenceClassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(model_name, num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Tokenize the prompts\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize_function\u001b[39m(examples):\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-Persönlich/programme/coding/github/ensure/.venv/lib/python3.11/site-packages/transformers/utils/import_utils.py:1690\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[0;34m(cls, key)\u001b[0m\n\u001b[1;32m   1688\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1689\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[0;32m-> 1690\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-Persönlich/programme/coding/github/ensure/.venv/lib/python3.11/site-packages/transformers/utils/import_utils.py:1678\u001b[0m, in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n\u001b[1;32m   1676\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[1;32m   1677\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[0;32m-> 1678\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\u001b[0;31mImportError\u001b[0m: \nAutoModelForSequenceClassification requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('prompts.csv', delimiter=';')\n",
    "\n",
    "# Load the tokenizer and model\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# Tokenize the prompts\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['TitlePrompt'], padding=\"max_length\", truncation=True)\n",
    "\n",
    "# Prepare the dataset\n",
    "dataset = df[['TitlePrompt']].dropna().reset_index(drop=True)\n",
    "dataset = dataset.rename(columns={'TitlePrompt': 'text'})\n",
    "dataset['labels'] = 0  # Dummy labels for fine-tuning\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_dataset = dataset.apply(tokenize_function, axis=1)\n",
    "\n",
    "# Convert to torch tensors\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "encodings = tokenizer(dataset['text'].tolist(), truncation=True, padding=True)\n",
    "labels = dataset['labels'].tolist()\n",
    "train_dataset = CustomDataset(encodings, labels)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained('./fine-tuned-model')\n",
    "tokenizer.save_pretrained('./fine-tuned-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
